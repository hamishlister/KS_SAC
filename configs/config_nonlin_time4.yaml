# environment and task
env:
  L: 11.6                             # Domain length  
  actuator_loss_weight: 1             # Weight for actuator loss
  seed: null                          # Random seed for reproducibility
  device: "mps"                       # Device to run the simulation on
  N: 32                               # Number of grid points
  dt: 0.01                            # Time step size  
  max_steps: 5000                     # Maximum number of steps per episode
  u0: null                            # Initial condition for the state
  lim: 1.0                            # Limit for the control input
  plot: False                         # Whether to plot the results
  verbose: True                       # Verbosity of the output
  info_freq: 5000                     # Frequency of printing information
  controller: 'nonlin'                # Type of controller to use
  sees_state: True                    # Whether the controller sees the full state
  observation_type: 'state_plus_time' # Type of observation to use
  reward_type: 'time'                 # Type of reward function to use
  pullback_state: True                # Whether to use pullback state
  noise: 0.0                          # Noise level for initial condition (if specified)
  initial_amp: 0.1                    # Amplitude of the initial condition
  continuous: False                   # Whether the environment is continuous or discrete

# model configuration
model:
  policy: 'LinearSACPolicy'   # Type of policy to use
  env: 'env'                  # Environment to use for the policy
  learning_rate: 0.0003       # Learning rate for the policy
  buffer_size: 200000         # Size of the replay buffer
  learning_starts: 50000      # Steps before learning starts
  batch_size: 256             # Batch size for training
  tau: 0.005                  # Soft update coefficient
  gamma: 0.99                 # Discount factor for future rewards
  train_freq: 1               # Frequency of training updates
  gradient_steps: 1           # Number of gradient steps per update
  action_noise: 0.0           # Noise added to actions for exploration
  ent_coef: 'auto'            # Entropy coefficient for exploration
  target_update_interval: 1   # Interval for updating the target network
  verbose: 1                  # Verbosity level for the model
  seed: null                  # Random seed for the model
  tensorboard_log: "./logs/"  # Directory for TensorBoard logs
  use_sde: False              # Whether to use State-Dependent Exploration
  use_sde_at_warmup: False    # Whether to use SDE during warmup
  policy_kwargs:              # Additional keyword arguments for the policy
    net_arch:                 # Network architecture
      pi: [256, 256, 256]     # Architecture of the policy network
      qf: [256, 256, 256]     # Architecture of the Q-function network
    activation_fn: 'nn.ReLU'  # Activation function to use in the network

# training configuration
train:
  total_timesteps: 10000000    # Total number of timesteps for training
  save_model: True             # Whether to save the model after training
  load_model: False            # Whether to load a pre-trained model
  num_models: 4                # Number of models to train


# logging
logger:
  mode: online
  eval_iter: 2  # Evaluate every x time steps
  num_eval_envs: 5
  num_train_envs: 5
  project_name: Time_reward
  team_name: s2030435-university-of-edinburgh
  save_replay_buffer: False
  
